```{r}
library(readr)
library(lubridate)
library(Metrics)
library(ggplot2)
library(dplyr)
library(e1071)

```
```{r}
umsatzdaten <- read_csv("https://raw.githubusercontent.com/opencampus-sh/wise20-datascience/main/umsatzdaten_gekuerzt.csv")

wetter <- read_csv("https://raw.githubusercontent.com/opencampus-sh/wise20-datascience/main/wetter.csv")

kiwo <- read_csv("https://raw.githubusercontent.com/opencampus-sh/wise20-datascience/main/kiwo.csv")

# der Link finktioniert manchmal nicht einfach in das Git gehen und suchen und neuladen
holidays <- read_csv("https://raw.githubusercontent.com/christophson3/Data-science-Team-8/main/Dataset_Public_holidays.csv?token=AR5YZ5WCSZECWQZFY22TNYC76XJBK")
```
```{r}

# Erstellung der Variable mit dem Wochentag
umsatzdaten$Wochentag <- weekdays(umsatzdaten$Datum)

#variabel Umsatz des Vortages
zumsatzdaten <- group_split(umsatzdaten,Warengruppe)

umsatzdaten1 <- mutate(zumsatzdaten[[1]], Vortagsumsatz = lag(Umsatz))
umsatzdaten2 <- mutate(zumsatzdaten[[2]], Vortagsumsatz = lag(Umsatz))
umsatzdaten3 <- mutate(zumsatzdaten[[3]], Vortagsumsatz = lag(Umsatz))
umsatzdaten4 <- mutate(zumsatzdaten[[4]], Vortagsumsatz = lag(Umsatz))
umsatzdaten5 <- mutate(zumsatzdaten[[5]], Vortagsumsatz = lag(Umsatz))
umsatzdaten6 <- mutate(zumsatzdaten[[6]], Vortagsumsatz = lag(Umsatz))

umsatzdaten <- full_join(umsatzdaten1,umsatzdaten2)
umsatzdaten <- full_join(umsatzdaten,umsatzdaten3)
umsatzdaten <- full_join(umsatzdaten,umsatzdaten4)
umsatzdaten <- full_join(umsatzdaten,umsatzdaten5)
umsatzdaten <- full_join(umsatzdaten,umsatzdaten6)

```
```{r}
# alle daten zusammengeführt
umsatzdaten_1 <- merge(umsatzdaten,wetter)
# left join für die Daten 
umsatzdaten_2 <- merge (x = umsatzdaten_1,y = holidays,all.x = TRUE)
umsatzdaten <- merge(x = umsatzdaten_2,y =kiwo, all.x = TRUE)
#View(umsatzdaten)


```
```{r}
# Aufraümen mit den Daten
# alle N.A von Vortagsumsatz löschen
umsatzdaten <- filter(umsatzdaten, !is.na(Vortagsumsatz))

# Holidays: auffüllen der nicht Feiertage + Löschen der unötigen Zeilen
#1 Feiertag mit hoher relevanz (Weihnachten, Ostern), 
#2 Feiertag mit weniger hoher Relevanz (Halloween), 
#3 Feiertag mit kaum Relevanz (Himmelfahrt und Totensonntag), 
#4 kein Feiertag

umsatzdaten <- mutate(umsatzdaten,Wertung = if (is.na(Wertung)){Wertung = 4})

# löschen von Event und Bedeutung
umsatzdaten <- subset(umsatzdaten,select = -c(Event,Beduetung))

#KIWO in TRUE und FALSE um rechnen
umsatzdaten <- mutate(umsatzdaten, KielerWoche = if (is.na(KielerWoche)){KielerWoche = FALSE} else {KielerWoche = TRUE})

# Wetterdaten: Wettercode: auffüllen der fehlenden Tage durch mittelwert

```

## Die SVM aus der VEranstaltung umgebastelt funktioniert bis jetzt noch nicht 
```{r}
## Aufteilung des Datensatzes in Trainings- und Testdaten

# Zufallszähler setzen (um die zufällige Partitionierung bei jedem Durchlauf gleich zu halten)
set.seed(1)

# Zufällige Ziehung von Indizes für die Zeilen des Datensatzes, die dem Traininsdatensatz zugeordnet werden
indices_train <- sample(seq_len(nrow(umsatzdaten)), size = floor(0.80 * nrow(umsatzdaten)))

# Definition des Trainings- und Testdatensatz durch Selektion bzw. Deselektion der entsprechenden Datenzeilen
train_dataset <- train_dataset_org <- umsatzdaten[indices_train, ]
test_dataset <- umsatzdaten[-indices_train, ]
```


## Data Preparation

```{r}
# Uncomment the next line if you want to check the correctness of your following code for the svm estimation with a small (and computationally fast) part of the training data set
#train_dataset <- sample_frac(train_dataset_org, .10)
```


## Training the SVM

```{r}
# Estimation of an SVM with optimized weighting parameters and given standard hyper parameters
# Typically not used; instead, the function svm_tune is used in order to also get a model with optimized hyper parameters
model_svm <- svm(Umsatz ~ Vortagumsatz, train_dataset)
```

```{r}
# Estimation of various SVM (each with optimized weighting parameters) using systematically varied hyper parameters (typically called 'grid search' approach) and cross validation
# the resulting object includes the optimal model in the element named 'best.model'
svm_tune <- tune(svm, Umsatz ~ Vortagumsatz + Temperatur + Wettercode + Bewoelkung, data=train_dataset,
                 ranges = list(epsilon = seq(0.2,1,0.1), cost = 2^(2:3)))
```


## Checking the Prediction Quality


### Trainig Data

SVM without cross validation and grid Search
```{r}
# Calculating the prediction for the training data using the best model according to the grid search
pred_train <- predict(model_svm, train_dataset)
# Calculating the prediction quality for the training data using the MAPE
mape(train_dataset$Umsatz, pred_train)

```

SVM with cross validation and grid Search
```{r}
# Calculating the prediction for the training data using the best model according to the grid search
pred_train <- predict(svm_tune$best.model, train_dataset)
# Calculating the prediction quality for the training data using the MAPE
mape(train_dataset$Umsatz, pred_train)
```

### Test Data

SVM without cross validation and grid Search
```{r}
# Calculating the prediction for the test data using the best model according to the grid search
pred_test <- predict(model_svm, test_dataset)
# Calculating the prediction quality for the test data using the MAPE
mape(test_dataset$Umsatz, pred_test)
```

SVM with cross validation and grid Search
```{r}
# Calculating the prediction for the test data using the best model according to the grid search
pred_test <- predict(svm_tune$best.model, test_dataset)
# Calculating the prediction quality for the test data using the MAPE
mape(test_dataset$Umsatz, pred_test)
```
